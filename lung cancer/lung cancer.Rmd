---
title       : ADVANCED R-FINAL PROJECT 
subtitle    : LUNG CANCER DETECTION
author      : Madanaganesh
job         : project
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : []            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides

---
  
  
## PROJECT OVERVIEW
  
1. Advanced R project to predict lung cancer in both Female and    male participants
2. Binomial Regression 
3. Comes under classification with yes/no output

---
  
## DATA INFORMATION
  
1. Data contains 229 instances
2. Has 9 predictor attributes and 1 predicted attribute 
3. Explaining attributes

-inst-(Numeric)

-time-(Numeric)

-wt.loss-(Numeric)	
                                         
-age-(Numeric)	
                                         
-sex-(logical)	
                                         
-ph.ecog- (Numeric)	
                                         
-ph.karno-(Numeric)
                                         
-pat.karno-(numeric)

-meal.cal-(numeric)
                                        
-Outcome-status-logical(YES or NO)

---

##Required libraries
```{r, warning=FALSE}
library(ROCR)
library(caret)
```

---

## READ DATA
```{r, warning=FALSE}
data<- read.csv("C:/Users/Madan/Documents/gp2.csv",header=TRUE, stringsAsFactors = TRUE)

data$status <- replace(data$status, data$status==1,1)
data$status <- replace(data$status, data$status==2,0)
data$status <- as.factor(data$status)

str(data)
```

---

##IMPUTE DATA using mean
```{r, echo=TRUE, message=FALSE, warning=FALSE}

library(mice)
md.pattern(data)#finding missing data
mecompdata <- mice(data,m=10,maxit=50,meth='pmm',seed=500)
mecompdata <- complete(mecompdata)
md.pattern(mecompdata)
sum(is.na(mecompdata))
str(mecompdata)
```

---
##EDA
```{r, warning=FALSE}
##Corr-plot
library(corrplot)
data1=mecompdata[-4]
corplot=cor(data1)
corplot1=corrplot(corplot,order="hclust",method="number")
##ph.ecog&ph.karno-high negative correlation
```

---
## EDA
```{r, warning=FALSE}
## Bar plot
library(ggplot2)
plot1=ggplot(mecompdata, aes(x =mecompdata$status)) + geom_bar()
plot1
```

---
## EDA
```{r, warning=FALSE}
##BOX-PLOTS
boxplot(mecompdata$inst, data=mecompdata)
boxplot(mecompdata$time, data=mecompdata)##has outliers
boxplot(mecompdata$age, data=mecompdata)
boxplot(mecompdata$ph.ecog, data=mecompdata)
boxplot(mecompdata$ph.karno, data=mecompdata)
boxplot(mecompdata$pat.karno, data=mecompdata)
boxplot(mecompdata$meal.cal, data=mecompdata)##has outliers
boxplot(mecompdata$wt.loss, data=mecompdata)##has outliers
```

---
## EDA
```{r, warning=FALSE}
hist(mecompdata$inst,col = "pink", freq = TRUE)
hist(mecompdata$time,col = "pink", freq = TRUE)##not linear
hist(mecompdata$age,col = "pink", freq = TRUE)
hist(mecompdata$ph.ecog,col = "pink", freq = TRUE)##not linear
hist(mecompdata$ph.karno,col = "pink", freq = TRUE)## not linear
hist(mecompdata$pat.karno,col = "pink", freq = TRUE)## notlinear
hist(mecompdata$meal.cal,col = "pink", freq = TRUE)
hist(mecompdata$wt.loss,col = "pink", freq = TRUE)
```

---

## EDA
```{r, warning=FALSE}
##scatter plots
pairs(~inst+time+age+ph.ecog,data=mecompdata, 
      main="Simple Scatterplot Matrix")
pairs(~ph.karno+pat.karno+meal.cal+wt.loss,data=mecompdata, 
      main="Simple Scatterplot Matrix")
pairs(~inst+ph.karno+meal.cal+ph.ecog,data=mecompdata, 
      main="Simple Scatterplot Matrix")
pairs(~ph.karno+time+meal.cal+ph.ecog,data=mecompdata, 
      main="Simple Scatterplot Matrix")
```

---
## DATA MANUPULATION
```{r, warning=FALSE}
##Manupulating outliers:limiting
mecompdata$time = pmin(mecompdata$time, 750)
mecompdata$meal.cal = pmin(mecompdata$meal.cal,1500)
mecompdata$meal.cal = pmax(mecompdata$meal.cal,350)
mecompdata$wt.loss = pmin(mecompdata$wt.loss,39)
mecompdata$X<-NULL
set.seed(17)
Train<- createDataPartition(mecompdata$status, p=0.7, list=FALSE)
nrow(mecompdata)
training <- mecompdata[ Train, ]
nrow(training)
testing <- mecompdata[ -Train, ]
nrow(testing)
```

---
## MODELING
```{r, warning=FALSE}
###plain glm model####
model_glm<-glm(status~.,data=training,family=binomial(link='logit'))
pred_glm<-predict(model_glm, newdata=testing)
cmatrix=table(predict(model_glm, newdata=testing,type='response')>0.5, testing$status)
dimnames(cmatrix)[[1]] = c("A","B")
colnames(cmatrix)[[1]] = c("A")
colnames(cmatrix)[[2]] = c("B")
Precision<-precision(cmatrix)
Recall<-recall(cmatrix)
F1 <- 2*Precision*Recall/(Precision+Recall)
Accuracy <- (cmatrix[1,1]+cmatrix[2,2])/sum(cmatrix)
Precision #0.9482759
Recall #0.9591837
F1 #0.8623853
Accuracy #0.7761194

perf_log <- prediction(pred_glm, testing$status)
roc <- performance(perf_log, "tpr", "fpr")
plot(roc, colorize = TRUE)
performance(perf_log, "auc")@y.values
```

---
##log transforming data
```{r, warning=FALSE}


Formula1=status~inst+log(time)+age+ph.ecog+log(ph.karno)+log(pat.karno)+meal.cal+wt.loss

model_glm1<-glm(Formula1,data=training,family=binomial(link='logit'))
pred_glm1<-predict(model_glm1, newdata=testing)
cmatrix=table(predict(model_glm1, newdata=testing,type='response')>0.5, testing$status)
dimnames(cmatrix)[[1]] = c("A","B")
colnames(cmatrix)[[1]] = c("A")
colnames(cmatrix)[[2]] = c("B")
Precision<-precision(cmatrix)
Recall<-recall(cmatrix)
F1 <- 2*Precision*Recall/(Precision+Recall)
Accuracy <- (cmatrix[1,1]+cmatrix[2,2])/sum(cmatrix)
Precision #0.7580645
Recall #0.9591837
F1 #0.8468468
Accuracy #0.7462687

perf_log1 <- prediction(pred_glm1, testing$status)
roc1 <- performance(perf_log1, "tpr", "fpr")
plot(roc1, colorize = TRUE)
performance(perf_log1, "auc")@y.values
```

---
##Feature Engineering
```{r, warning=FALSE}
fecompdata=mecompdata
fecompdata$att1 <- mecompdata$ph.ecog^2
fecompdata$att2 <- mecompdata$ph.ecog^3
fecompdata$att3 <- mecompdata$ph.karno^2
fecompdata$att4 <- mecompdata$ph.karno^3
fecompdata$att5 <- mecompdata$pat.karno^2
fecompdata$att6 <- mecompdata$pat.karno^3
fecompdata$att7 <- mecompdata$ph.ecog*mecompdata$ph.karno
fecompdata$att8 <- mecompdata$ph.karno*mecompdata$pat.karno
fecompdata$att9 <- mecompdata$ph.ecog*mecompdata$pat.karno
fecompdata$att10 <- mecompdata$ph.ecog*(mecompdata$ph.karno^2)
fecompdata$att11 <- (mecompdata$ph.ecog^2)*mecompdata$ph.karno
fecompdata$att12 <- mecompdata$ph.karno*(mecompdata$pat.karno^2)
fecompdata$att13 <- (mecompdata$ph.karno^2)*mecompdata$pat.karno
fecompdata$att14 <- mecompdata$ph.ecog*(mecompdata$pat.karno^2)
fecompdata$att15 <- (mecompdata$ph.ecog^2)*mecompdata$pat.karno
fecompdata$att16 <- mecompdata$meal.cal^2
fecompdata$att17 <- mecompdata$meal.cal^3
fecompdata$att18 <- mecompdata$wt.loss^2
fecompdata$att19 <- mecompdata$wt.loss^3
fecompdata$att20 <- mecompdata$age^2
fecompdata$att21 <- mecompdata$age^3
fecompdata$att22 <- mecompdata$meal.cal*mecompdata$wt.loss
fecompdata$att23 <- mecompdata$wt.loss*mecompdata$age
fecompdata$att24 <- mecompdata$meal.cal*mecompdata$age
fecompdata$att25 <- mecompdata$meal.cal*(mecompdata$wt.loss^2)
fecompdata$att26 <- (mecompdata$meal.cal^2)*mecompdata$wt.loss
fecompdata$att27 <- mecompdata$wt.loss*(mecompdata$age^2)
fecompdata$att28 <- (mecompdata$wt.loss^2)*mecompdata$age
fecompdata$att29 <- mecompdata$meal.cal*(mecompdata$age^2)
fecompdata$att30 <- (mecompdata$meal.cal^2)*mecompdata$age

set.seed(17)
Train<- createDataPartition(fecompdata$status, p=0.7, list=FALSE)
nrow(fecompdata)
training <- fecompdata[ Train, ]
nrow(training)
testing <- fecompdata[ -Train, ]
nrow(testing)

Formula2=status~inst+log(time)+age+ph.ecog+log(ph.karno)+log(pat.karno)+meal.cal+wt.loss+att1+att2+att3+att4+att5+att6+att7+att8+att9+att10+att11+att12+att13+att14+att15+att16+att17+att18+att19+att20+att21+att22+att23+att24+att25+att26+att27+att28+att29+att30

model_glm2<-glm(Formula2,data=training,family=binomial(link='logit'))
pred_glm2<-predict(model_glm2, newdata=testing)
cmatrix=table(predict(model_glm2, newdata=testing,type='response')>0.5, testing$status)
dimnames(cmatrix)[[1]] = c("A","B")
colnames(cmatrix)[[1]] = c("A")
colnames(cmatrix)[[2]] = c("B")
Precision<-precision(cmatrix)
Recall<-recall(cmatrix)
F1 <- 2*Precision*Recall/(Precision+Recall)
Accuracy <- (cmatrix[1,1]+cmatrix[2,2])/sum(cmatrix)
Precision #0.7169811
Recall #0.7755102
F1 #0.745098
Accuracy #0.6119403###heavy dip in accuracy

perf_log2 <- prediction(pred_glm2, testing$status)
roc2 <- performance(perf_log2, "tpr", "fpr")
plot(roc2, colorize = TRUE)
performance(perf_log2, "auc")@y.values
```

---
##Principal component analysis
```{r, warning=FALSE}
str(fecompdata)
##removing dependent variable
pca.test <-testing[,-3]
pca.train <- training[,-3]

##applying PCA
prin_comp <- prcomp(pca.train, scale. = T)

##Printing rotation and std-dev
prin_comp$scale
prin_comp$rotation

##first 5 rows of 4 pc's
prin_comp$rotation[1:5,1:4]
##plot
biplot(prin_comp, scale = 0)
##std_dev
std_dev <- prin_comp$sdev
pr_var <- std_dev^2

#check variance of first 10 components
pr_var[1:10]
prop_varex <- pr_var/sum(pr_var)
prop_varex[1:20]

##screeplot- first 11 PC'S EXPLAIN 95% OF VARIATION
plot(prop_varex, xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     type = "b")

##This confirms the above infrence
plot(cumsum(prop_varex), xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     type = "b")

##prediction
train.data <- data.frame(status= training$status, prin_comp$x)

#select first 10 PCAs
train.data <- train.data[,1:11]

##run a model
model_glm4 <- glm(status ~ .,data = train.data,family=binomial(link='logit'))
model_glm4

#transform test into PCA
test.data <- predict(prin_comp, newdata = pca.test)
test.data <- as.data.frame(test.data)


#select the first 11 components
test.data <- test.data[,1:11]
test.data$status <-testing$status
test.data <- as.data.frame(test.data)

#make prediction on test data
pred_glm4 <- predict(model_glm4, test.data)

cmatrix=table(pred_glm4>0.5, test.data$status)
dimnames(cmatrix)[[1]] = c("A","B")
colnames(cmatrix)[[1]] = c("A")
colnames(cmatrix)[[2]] = c("B")
Precision<-precision(cmatrix)
Recall<-recall(cmatrix)

F1 <- 2*Precision*Recall/(Precision+Recall)
Accuracy <- (cmatrix[1,1]+cmatrix[2,2])/sum(cmatrix)
Precision #0.7741935
Recall #0.9795918
F1 #0.8648649
Accuracy #0.7761194##improved accuracy


perf_log4 <- prediction(pred_glm4, test.data$status)
roc4 <- performance(perf_log4, "tpr", "fpr")
plot(roc4, colorize = TRUE)
performance(perf_log4, "auc")@y.values
```

---
##stewpise selection
```{r, warning=FALSE}

fecompdata=mecompdata
fecompdata$att1 <- mecompdata$ph.ecog^2
fecompdata$att2 <- mecompdata$ph.ecog^3
fecompdata$att3 <- mecompdata$ph.karno^2
fecompdata$att4 <- mecompdata$ph.karno^3
fecompdata$att5 <- mecompdata$pat.karno^2
fecompdata$att6 <- mecompdata$pat.karno^3
fecompdata$att7 <- mecompdata$ph.ecog*mecompdata$ph.karno
fecompdata$att8 <- mecompdata$ph.karno*mecompdata$pat.karno
fecompdata$att9 <- mecompdata$ph.ecog*mecompdata$pat.karno
fecompdata$att10 <- mecompdata$ph.ecog*(mecompdata$ph.karno^2)
fecompdata$att11 <- (mecompdata$ph.ecog^2)*mecompdata$ph.karno
fecompdata$att12 <- mecompdata$ph.karno*(mecompdata$pat.karno^2)
fecompdata$att13 <- (mecompdata$ph.karno^2)*mecompdata$pat.karno
fecompdata$att14 <- mecompdata$ph.ecog*(mecompdata$pat.karno^2)
fecompdata$att15 <- (mecompdata$ph.ecog^2)*mecompdata$pat.karno
fecompdata$att16 <- mecompdata$meal.cal^2
fecompdata$att17 <- mecompdata$meal.cal^3
fecompdata$att18 <- mecompdata$wt.loss^2
fecompdata$att19 <- mecompdata$wt.loss^3
fecompdata$att20 <- mecompdata$age^2
fecompdata$att21 <- mecompdata$age^3
fecompdata$att22 <- mecompdata$meal.cal*mecompdata$wt.loss
fecompdata$att23 <- mecompdata$wt.loss*mecompdata$age
fecompdata$att24 <- mecompdata$meal.cal*mecompdata$age
fecompdata$att25 <- mecompdata$meal.cal*(mecompdata$wt.loss^2)
fecompdata$att26 <- (mecompdata$meal.cal^2)*mecompdata$wt.loss
fecompdata$att27 <- mecompdata$wt.loss*(mecompdata$age^2)
fecompdata$att28 <- (mecompdata$wt.loss^2)*mecompdata$age
fecompdata$att29 <- mecompdata$meal.cal*(mecompdata$age^2)
fecompdata$att30 <- (mecompdata$meal.cal^2)*mecompdata$age

set.seed(17)
Train<- createDataPartition(fecompdata$status, p=0.7, list=FALSE)
nrow(fecompdata)
training <- fecompdata[ Train, ]
nrow(training)
testing <- fecompdata[ -Train, ]
nrow(testing)


null<-glm(status~1,data=training,family=binomial(link='logit'))
full<-glm(status~.,data=training,family=binomial(link='logit'))
step(null, scope=list(lower=null, upper=full), direction="forward")


model_glm5<-glm(status~ inst+sex+ph.ecog+age,data=training,family=binomial(link='logit'))
pred_glm5<-predict(model_glm5, newdata=testing)

cmatrix=table(predict(model_glm5, newdata=testing,type='response')>0.5, testing$status)
dimnames(cmatrix)[[1]] = c("A","B")
colnames(cmatrix)[[1]] = c("A")
colnames(cmatrix)[[2]] = c("B")
Precision<-precision(cmatrix)
Recall<-recall(cmatrix)
F1 <- 2*Precision*Recall/(Precision+Recall)
Accuracy <- (cmatrix[1,1]+cmatrix[2,2])/sum(cmatrix)
Precision #0.7931034
Recall #0.9387755
F1 #0.8598131
Accuracy #0.7761194###same accuracy as PCA

perf_log5 <- prediction(pred_glm5, testing$status)
roc5 <- performance(perf_log5, "tpr", "fpr")
plot(roc5, colorize = TRUE)
performance(perf_log5, "auc")@y.values
```
---

## ROC CURVE
```{r, warning=FALSE}
plot(roc, colorize=TRUE)
plot(roc1, add = TRUE, colorize = TRUE)
plot(roc2, add = TRUE, colorize = TRUE)
plot(roc4, add = TRUE, colorize = TRUE)##BETTER ROCR
plot(roc5, add = TRUE, colorize = TRUE)
```
-----




